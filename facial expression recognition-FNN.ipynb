{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "costtime is22.82733654975891s\n"
     ]
    }
   ],
   "source": [
    "#import pandas as pd  \n",
    "import numpy as np  \n",
    "import time\n",
    "a=time.time()\n",
    "Y = []\n",
    "X = []\n",
    "first = True\n",
    "for line in open('./data/fer2013.csv'):\n",
    "    if first:\n",
    "        first = False\n",
    "    else:\n",
    "        row = line.split(',')\n",
    "        Y.append(int(row[0]))\n",
    "        X.append([int(p) for p in row[1].split()])\n",
    "X, Y = np.array(X), np.array(Y)\n",
    "b=time.time()\n",
    "print(\"costtime is\"+str(b-a)+\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[Y>0]=Y[Y>0]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35887, 2304)\n",
      "(35887,)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "y_train = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35887, 6)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "X=X.reshape([-1,48,48,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34000, 6)\n",
      "(34000, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x=X[0:34000]\n",
    "train_y=y_train[0:34000]\n",
    "test_x=X[0:1300]\n",
    "test_y=y_train[0:1300]\n",
    "print(train_y.shape)\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x / 255.\n",
    "test_x = test_x / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(X_shape, y_shape, batch_size, dropout, nb_epoch, conv_arch, dense):\n",
    "    print (' X_train shape: ', X_shape )# (n_sample, 1, 48, 48)\n",
    "    print (' y_train shape: ', y_shape) # (n_sample, n_categories)\n",
    "    print ('      img size: ', X_shape[2], X_shape[3])\n",
    "    print ('    batch size: ', batch_size)\n",
    "    print ('      nb_epoch: ', nb_epoch)\n",
    "    print ('       dropout: ', dropout)\n",
    "    print ('conv architect: ', conv_arch)\n",
    "    print ('neural network: ', dense)\n",
    "\n",
    "def logging(model, starttime, batch_size, nb_epoch, conv_arch,dense, dropout,\n",
    "            X_shape, y_shape, train_acc, val_acc, dirpath):\n",
    "    now = time.ctime()\n",
    "    model.save_weights('./data/weights/p'+str(time.time()/1000))\n",
    "    save_model(model.to_json(), now, dirpath)\n",
    "    save_config(model.get_config(), now, dirpath)\n",
    "    save_result(starttime, batch_size, nb_epoch, conv_arch, dense, dropout,\n",
    "                    X_shape, y_shape, train_acc, val_acc, dirpath)\n",
    "\n",
    "def cnn_architecture(X_train, y_train, conv_arch=[(32,3),(64,3),(128,3)],\n",
    "                    dense=[64,2], dropout=0.3, batch_size=128, nb_epoch=100, validation_split=0.02, patience=5, dirpath='./data/results/'):\n",
    "    starttime = time.time()\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_shape = X_train.shape\n",
    "    y_shape = y_train.shape\n",
    "    describe(X_shape, y_shape, batch_size, dropout, nb_epoch, conv_arch, dense)\n",
    "\n",
    "    # data augmentation:\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=validation_split)\n",
    "    # datagen = ImageDataGenerator(rescale=1./255,\n",
    "    #                              rotation_range=10,\n",
    "    #                              shear_range=0.2,\n",
    "    #                              width_shift_range=0.2,\n",
    "    #                              height_shift_range=0.2,\n",
    "    #                              horizontal_flip=True)\n",
    "\n",
    "    # datagen.fit(X_train)\n",
    "    # model architecture:\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(conv_arch[0][0], 3, 3, border_mode='same', activation='relu',input_shape=( X_train.shape[1], X_train.shape[2],1)))\n",
    "#   model.add(Conv2D(conv_arch[0][0], (3, 3), activation=\"relu\", input_shape=( X_train.shape[1], X_train.shape[2],1, padding=\"same\")))\n",
    "    if (conv_arch[0][1]-1) != 0:\n",
    "        for i in range(conv_arch[0][1]-1):\n",
    "            model.add(Convolution2D(conv_arch[0][0], 3, 3, border_mode='same', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    if conv_arch[1][1] != 0:\n",
    "        for i in range(conv_arch[1][1]):\n",
    "            model.add(Convolution2D(conv_arch[1][0], 3, 3, border_mode='same', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    if conv_arch[2][1] != 0:\n",
    "        for i in range(conv_arch[2][1]):\n",
    "            model.add(Convolution2D(conv_arch[2][0], 3, 3, border_mode='same', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())  # this converts 3D feature maps to 1D feature vectors\n",
    "    if dense[1] != 0:\n",
    "        for i in range(dense[1]):\n",
    "            model.add(Dense(dense[0], activation='relu'))\n",
    "            if dropout:\n",
    "                model.add(Dropout(dropout))\n",
    "    prediction = model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "    # optimizer:\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # set callback:\n",
    "    callbacks = []\n",
    "#    if patience != 0:\n",
    " #       early_stopping = EarlyStopping(monitor='val_loss', patience=patience, verbose=1)\n",
    " #       callbacks.append(early_stopping)\n",
    "\n",
    "    print ('Training....')\n",
    "    # fits the model on batches with real-time data augmentation:\n",
    "    # hist = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "    #                 samples_per_epoch=len(X_train), nb_epoch=nb_epoch, validation_data=(X_test,y_test), callbacks=callbacks, verbose=1)\n",
    "\n",
    "    '''without data augmentation'''\n",
    "    hist = model.fit(X_train, y_train, nb_epoch=nb_epoch, batch_size=batch_size,\n",
    "              validation_split=validation_split, callbacks=callbacks, shuffle=True, verbose=1)\n",
    "\n",
    "    # model result:\n",
    "    train_val_accuracy = hist.history\n",
    "    train_acc = train_val_accuracy['acc']\n",
    "    val_acc = train_val_accuracy['val_acc']\n",
    "    print ('          Done!')\n",
    "#    print ('     Train acc: '+train_acc[-1])\n",
    "#    print ('Validation acc: '+val_acc[-1])\n",
    "#    print (' Overfit ratio: '+ val_acc[-1]/train_acc[-1])\n",
    "\n",
    "#    logging(model, starttime, batch_size, nb_epoch, conv_arch, dense,\n",
    "#            dropout, X_shape, y_shape, train_acc, val_acc, dirpath)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X_train shape:  (34000, 48, 48, 1)\n",
      " y_train shape:  (34000, 6)\n",
      "      img size:  48 1\n",
      "    batch size:  128\n",
      "      nb_epoch:  30\n",
      "       dropout:  0.45\n",
      "conv architect:  [(32, 3), (64, 3), (128, 3)]\n",
      "neural network:  [64, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:40: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(48, 48, 1..., padding=\"same\")`\n",
      "D:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "D:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:49: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "D:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:54: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33320 samples, validate on 680 samples\n",
      "Epoch 1/30\n",
      " 5888/33320 [====>.........................] - ETA: 7:15 - loss: 1.7743 - acc: 0.2458"
     ]
    }
   ],
   "source": [
    "model=cnn_architecture(train_x, train_y, conv_arch=[(32,3),(64,3),(128,3)], dense=[64,2], dropout=0.45, batch_size=128, nb_epoch=30, dirpath = './data/results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
